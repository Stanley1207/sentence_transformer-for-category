# -*- coding: utf-8 -*-
"""classify_enterprises.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171-Tevx97tDWgxw4bf81rsCxIBS3-OMM
"""

import pandas as pd
import numpy as np
import os
import torch
import re
from collections import OrderedDict
from typing import List, Dict
from sentence_transformers import SentenceTransformer
import warnings

warnings.filterwarnings("ignore")

MODEL_NAME = "infgrad/stella-mrl-large-zh-v3.5-1792d"

# ===================== 分类体系定义 =====================
# 每个分类包含：名称 + 描述性文本（用于生成向量）

CLASSIFICATION_SYSTEM = OrderedDict([
    ("数据资源企业", OrderedDict([
        ("公共数据资源企业", {
            "keywords": ["公共数据", "政府数据", "公共服务", "电力数据", "水务数据", "燃气数据",
                        "公共交通数据", "政务数据", "公共资源", "公共机构", "公共服务数据"],
            "description": "提供公共数据资源服务，包括政府数据、公共服务数据、水电燃气等公共事业数据的采集、管理和应用"
        }),
        ("行业数据资源企业", {
            "keywords": ["金融数据", "通信数据", "能源数据", "行业数据", "证券数据", "保险数据",
                        "银行数据", "电信数据", "运营商数据", "行业龙头", "垂直行业数据"],
            "description": "提供特定行业的数据资源服务，如金融、通信、能源等垂直行业的数据采集和管理"
        }),
        ("互联网数据资源企业", {
            "keywords": ["互联网平台", "用户行为数据", "流量数据", "社交数据", "电商数据",
                        "旅行平台", "外卖平台", "出行平台", "直播平台", "短视频平台",
                        "C端用户", "移动端", "物联网数据", "互联网公开数据"],
            "description": "提供互联网平台数据资源，包括用户行为数据、流量数据、社交媒体数据、电商数据等线上数据资源"
        }),
        ("数据资源集成企业", {
            "keywords": ["数据集成", "数据汇聚", "数据聚合", "API数据", "数据采购",
                        "数据清洗整合", "标准化数据集", "数据产品", "多方数据", "聚合数据"],
            "description": "整合多来源数据资源，提供数据集成、数据聚合、API接口等数据资源整合服务"
        }),
    ])),
    ("数据技术企业", OrderedDict([
        ("数据采集汇聚技术企业", {
            "keywords": ["数据采集", "数据抓取", "爬虫", "数据接口", "传感器", "物联网采集",
                        "数据收集", "数据汇聚", "智能传感", "数据爬取", "采集工具", "采集平台"],
            "description": "提供数据采集和汇聚技术服务，包括网络爬虫、传感器采集、物联网数据采集、API接口对接等技术"
        }),
        ("数据治理应用技术企业", {
            "keywords": ["数据清洗", "数据标准化", "数据标注", "数据结构化", "数据质量",
                        "数据治理", "主数据管理", "元数据管理", "数据标签", "数据规范",
                        "数据架构", "数据一致性", "数据准确性"],
            "description": "提供数据治理技术服务，包括数据清洗、数据标准化、数据质量管理、元数据管理、主数据管理等"
        }),
        ("数据加工分析技术企业", {
            "keywords": ["用户画像", "商业智能", "BI", "可视化分析", "数据可视化", "智能预测",
                        "大数据分析", "数据挖掘", "舆情监测", "舆情分析", "数据报表",
                        "决策支持", "数据洞察", "数据建模", "预测分析", "数据压缩"],
            "description": "提供数据分析和加工技术，包括商业智能BI、数据可视化、用户画像、数据挖掘、预测分析等"
        }),
        ("人工智能技术企业", {
            "keywords": ["人工智能", "AI", "机器学习", "深度学习", "计算机视觉", "图像识别",
                        "自然语言处理", "NLP", "语音识别", "语音合成", "智能问答",
                        "知识图谱", "神经网络", "大模型", "GPT", "AIGC", "生成式AI",
                        "ChatGPT", "自动驾驶", "智能推荐", "算法模型"],
            "description": "提供人工智能技术服务，包括机器学习、深度学习、计算机视觉、自然语言处理、大模型、AIGC等AI技术研发和应用"
        }),
        ("数据全链技术企业", {
            "keywords": ["大数据平台", "数据全生命周期", "全栈数据", "数据中台", "数据湖",
                        "数据仓库", "端到端", "一站式数据", "全链路", "数据全链",
                        "数据服务平台", "PaaS", "数据平台"],
            "description": "提供数据全生命周期技术服务，包括数据中台、数据湖、数据仓库、大数据平台等端到端数据技术解决方案"
        }),
    ])),
    ("数据服务企业", OrderedDict([
        ("数据经纪", {
            "keywords": ["数据经纪", "数据中介", "数据代理", "数据撮合", "数据交易撮合",
                        "数据流通", "数据代销"],
            "description": "提供数据经纪服务，作为数据交易中介，撮合数据供需双方，促进数据流通"
        }),
        ("数据咨询", {
            "keywords": ["数字化转型咨询", "数据咨询", "数据战略", "数据管理体系",
                        "数字化咨询", "数据规划", "IT咨询", "信息化咨询"],
            "description": "提供数据咨询服务，包括数字化转型咨询、数据战略规划、数据管理体系建设等咨询服务"
        }),
        ("法律合规", {
            "keywords": ["数据合规", "数据安全评估", "合规评估", "数据产权", "数据登记",
                        "数据仲裁", "数据权益", "隐私合规", "GDPR", "数据保护"],
            "description": "提供数据法律合规服务，包括数据合规评估、隐私保护、数据产权登记、数据安全评估等法律服务"
        }),
        ("资产评估", {
            "keywords": ["数据资产评估", "数据估值", "资产入表", "数据定价", "价值评估"],
            "description": "提供数据资产评估服务，包括数据估值、数据资产入表、数据定价等评估服务"
        }),
        ("数据交易", {
            "keywords": ["数据交易", "数据交易所", "数据交易平台", "数据买卖", "数据挂牌",
                        "数据结算", "数据登记"],
            "description": "提供数据交易服务，运营数据交易平台或交易所，支持数据产品挂牌、交易、结算"
        }),
    ])),
    ("数据应用企业", OrderedDict([
        ("工业制造应用", {
            "keywords": ["工业数据", "智能制造", "工业互联网", "工业4.0", "制造业数字化",
                        "生产制造", "供应链管理", "设备运维", "工业软件", "MES", "PLM",
                        "汽车数据", "工业设计", "研发设计", "质量管理", "生产管控"],
            "description": "数据在工业制造领域的应用，包括智能制造、工业互联网、生产管控、设备运维、供应链管理等工业数字化应用"
        }),
        ("现代农业应用", {
            "keywords": ["农业数据", "智慧农业", "数字农业", "农产品溯源", "种养殖",
                        "农业生产", "数智种业", "精准农业", "农业物联网"],
            "description": "数据在农业领域的应用，包括智慧农业、精准农业、农产品溯源、种养殖管理、农业物联网等农业数字化应用"
        }),
        ("商贸流通应用", {
            "keywords": ["商贸数据", "物流数据", "供应链", "仓储", "配送", "电商",
                        "跨境贸易", "跨境电商", "新零售", "商贸流通", "分销", "批发"],
            "description": "数据在商贸流通领域的应用，包括电商、物流、仓储、供应链、跨境贸易、新零售等商贸数字化应用"
        }),
        ("交通运输应用", {
            "keywords": ["交通数据", "智慧交通", "智能交通", "运输管理", "出行服务",
                        "车联网", "船舶数据", "航运", "物流运输", "公共交通", "交通规划"],
            "description": "数据在交通运输领域的应用，包括智慧交通、车联网、出行服务、物流运输、航运等交通数字化应用"
        }),
        ("金融服务应用", {
            "keywords": ["金融科技", "风控", "风险管理", "信贷", "征信", "反欺诈",
                        "数字金融", "智能投顾", "量化交易", "保险科技", "支付"],
            "description": "数据在金融领域的应用，包括金融科技、风控、征信、智能投顾、保险科技、支付等金融数字化应用"
        }),
        ("科技创新应用", {
            "keywords": ["科技研发", "研发管理", "创新管理", "专利分析", "技术转化",
                        "科研数据", "实验数据", "研发效能", "数据智能体"],
            "description": "数据在科技创新领域的应用，包括研发管理、专利分析、技术转化、科研数据管理等科技创新数字化应用"
        }),
        ("文化旅游应用", {
            "keywords": ["文旅数据", "智慧旅游", "智慧文旅", "旅游推荐", "客流分析",
                        "智慧导览", "沉浸式体验"],
            "description": "数据在文化旅游领域的应用，包括智慧旅游、智慧导览、客流分析、旅游推荐等文旅数字化应用"
        }),
        ("医疗健康应用", {
            "keywords": ["医疗数据", "健康数据", "智慧医疗", "医疗AI", "医疗影像",
                        "健康管理", "医药研发", "临床数据", "电子病历", "数字医疗",
                        "远程医疗", "医疗信息化"],
            "description": "数据在医疗健康领域的应用，包括智慧医疗、医疗AI、健康管理、医药研发、远程医疗等医疗数字化应用"
        }),
        ("应急管理应用", {
            "keywords": ["应急管理", "灾害监测", "预警系统", "应急指挥", "防灾减灾",
                        "安全生产", "消防", "应急响应", "灾害预警"],
            "description": "数据在应急管理领域的应用，包括灾害监测预警、应急指挥、安全生产、消防等应急管理数字化应用"
        }),
        ("气象服务应用", {
            "keywords": ["气象数据", "气象服务", "天气预报", "气候分析", "气象监测",
                        "农业气象", "航空气象"],
            "description": "数据在气象服务领域的应用，包括天气预报、气候分析、气象监测、农业气象等气象数字化应用"
        }),
        ("城市治理应用", {
            "keywords": ["智慧城市", "城市大脑", "城市治理", "政务服务", "一网通办",
                        "一网统管", "数字政府", "基层治理", "社区管理", "城市运行"],
            "description": "数据在城市治理领域的应用，包括智慧城市、城市大脑、数字政府、政务服务、社区管理等城市数字化应用"
        }),
        ("绿色低碳应用", {
            "keywords": ["碳排放", "碳中和", "碳达峰", "双碳", "环保数据", "绿色能源",
                        "能源管理", "节能减排", "环境监测", "污染监测", "碳交易"],
            "description": "数据在绿色低碳领域的应用，包括碳排放管理、能源管理、环境监测、碳交易等绿色低碳数字化应用"
        }),
        ("综合型数据应用", {
            "keywords": ["跨行业", "多领域", "综合数据", "通用数据", "多场景", "一站式"],
            "description": "跨行业跨领域的综合型数据应用，提供多场景、多领域的通用数据应用服务"
        }),
        ("人工智能应用", {
            "keywords": ["AI教育", "智能教育", "AI医疗", "AI金融", "AI制造",
                        "行业AI应用", "垂直AI", "AI+", "人工智能教育"],
            "description": "人工智能在各行业的垂直应用，包括AI教育、AI医疗、AI金融、AI制造等行业AI应用"
        }),
        ("低空经济应用", {
            "keywords": ["无人机", "低空经济", "低空飞行", "空域管理", "无人机物流",
                        "飞行器", "eVTOL", "低空管理"],
            "description": "数据在低空经济领域的应用，包括无人机、低空飞行管理、空域管理、无人机物流等低空经济数字化应用"
        }),
    ])),
    ("数据安全企业", OrderedDict([
        ("综合型数据安全企业", {
            "keywords": ["网络安全", "信息安全", "数据安全", "安全防护", "安全服务",
                        "安全运营", "安全检测", "漏洞扫描", "入侵检测", "防火墙",
                        "终端安全", "云安全", "安全审计"],
            "description": "提供综合性数据安全服务，包括网络安全、信息安全、安全防护、安全运营、安全检测等全方位安全服务"
        }),
        ("专精型数据安全企业", {
            "keywords": ["隐私计算", "多方计算", "联邦学习", "可信执行", "数据脱敏",
                        "数据加密", "密码技术", "身份认证", "访问控制", "数据防泄漏",
                        "DLP", "零信任"],
            "description": "提供专业化数据安全技术，包括隐私计算、联邦学习、数据加密、身份认证、数据防泄漏等专精安全技术"
        }),
    ])),
    ("数据基础设施企业", OrderedDict([
        ("数据流通利用设施企业", {
            "keywords": ["可信数据空间", "数据元件", "数联网", "区块链", "隐私保护平台",
                        "数据流通基础设施", "数场", "分布式账本"],
            "description": "提供数据流通基础设施，包括可信数据空间、区块链、数联网、隐私保护平台等数据流通基础设施"
        }),
        ("数据存算设施企业", {
            "keywords": ["数据中心", "云计算", "存储", "计算设施", "算力", "超算",
                        "服务器", "数据存储", "分布式存储", "GPU集群", "智算中心"],
            "description": "提供数据存储和计算基础设施，包括数据中心、云计算、服务器、存储、算力、超算等基础设施"
        }),
        ("网络设施企业", {
            "keywords": ["网络基础设施", "5G", "通信网络", "光网络", "网络设备",
                        "交换机", "路由器", "超融合", "网络传输", "SDN"],
            "description": "提供网络基础设施，包括5G、通信网络、光网络、网络设备、SDN等网络基础设施"
        }),
    ])),
])


# ===================== 资质和特殊备注（保持原有逻辑）=====================
QUALIFICATION_KEYWORDS = {
    "专精特新": ["专精特新", "小巨人"],
    "高新技术企业": ["高新技术企业", "高新企业", "国家级高新"],
    "独角兽企业": ["独角兽"],
    "瞪羚企业": ["瞪羚"],
}

SPECIAL_KEYWORDS = {
    "人工智能": ["人工智能", "AI", "机器学习", "深度学习", "大模型", "AIGC"],
    "低空经济": ["无人机", "低空经济", "低空飞行", "eVTOL"],
    "平台经济": ["平台经济", "互联网平台", "电商平台", "共享经济"],
    "云计算": ["云计算", "云服务", "SaaS", "PaaS", "IaaS", "云平台"],
    "区块链": ["区块链", "分布式账本", "智能合约", "Web3"],
    "半导体": ["半导体", "芯片", "集成电路", "芯片设计", "芯片制造"],
    "元宇宙": ["元宇宙", "虚拟现实", "增强现实", "VR", "AR"],
    "数字孪生": ["数字孪生", "Digital Twin", "虚拟仿真"],
    "物联网": ["物联网", "IoT", "传感器网络", "智能设备"],
    "大数据": ["大数据", "数据分析", "数据挖掘", "数据治理"],
    "5G通信": ["5G", "5G网络", "通信技术", "移动通信"],
    "机器人": ["机器人", "智能机器人", "服务机器人", "工业机器人"],
    "绿色低碳": ["碳中和", "碳达峰", "绿色能源", "环保技术"],
    "量子计算": ["量子计算", "量子通信", "量子技术"],
}

# def get_source_strategy(source):
#     """获取来源对应的处理策略"""
#     if pd.isna(source) or not source:
#         return "reclassify"  # 默认重新分类
#     return SOURCE_STRATEGY.get(str(source).strip(), "reclassify")

def clean_enterprise_description(desc, company_name):
    """
    清洗企业简介：
    1. 删除公司名称开头
    2. 删除法定代表人/法人/董事长信息
    3. 删除成立日期信息
    4. 删除AI生成声明
    5. 删除公司地址
    """
    if pd.isna(desc) or not desc:
        return ""

    desc = str(desc).strip()

    # 删除AI生成声明
    desc = re.sub(r'以上内容由AI生成[，,]?仅供参考[，,]?请注意甄别[。.]?', '', desc)

    # 删除公司名称开头（支持多种格式）
    if company_name and not pd.isna(company_name):
        # 转义公司名中的特殊字符
        escaped_name = re.escape(str(company_name))
        desc = re.sub(rf'^{escaped_name}[，,]?\s*', '', desc)

    # 删除成立日期相关信息（更精确的匹配）
    # desc = re.sub(r'成立于\d{4}[-年]\d{1,2}[-月]\d{1,2}日?[，,]?\s*', '', desc)
    # desc = re.sub(r'成立于\d{4}年\d{1,2}月[，,]?\s*', '', desc)
    desc = re.sub(r'成立于\d{4}(?:[-年/\.](?:\d{1,2}(?:[-月/\.](?:\d{1,2}日?)?)?)?)?[，,。]?\s*', '', desc)

    # 删除法定代表人/法人/董事长信息
    desc = re.sub(r'法定?代表人为?[^，,。]+[，,]?\s*', '', desc)
    desc = re.sub(r'董事长为?[^，,。]+[，,]?\s*', '', desc)
    desc = re.sub(r'负责人[是否为：:]?\s*[^，,。]+[，,]?\s*', '', desc)
    # 匹配：由 + 姓名 + 担任/出任 + 职位 + (可选标点)
    desc = re.sub(r'由[^，,。]+(?:担任|出任|兼任)(?:董事长|总经理|法定代表人|负责人|CEO|执行董事)[。，,]?\s*', '', desc)

    # 删除注册资本信息
    # desc = re.sub(r'注册资本为?\d+[万亿]?元?[，,]?\s*', '', desc)
    desc = re.sub(r'(?:注册|实缴)资本(?:为)?[\d\.]+[万亿]?(?:元|人民币|美元|HKD|USD)?[，,]?\s*', '', desc)

    # 删除统一社会信用代码
    # desc = re.sub(r'统一社会信用代码为?[A-Z0-9]+[，,]?\s*', '', desc)
    desc = re.sub(r'统一社会信用代码为?[A-Z0-9]+[，,。]?\s*', '', desc)

    # 删除注册地址
    # desc = re.sub(r'企业注册地址位于[^，,。]+[，,]?\s*', '', desc)
    # desc = re.sub(r'公司位于[^，,。]+[，,]?\s*', '', desc)
    desc = re.sub(r'(?:企业注册地址|公司地址|位于|坐落于)[^，,。]+[，,。]?\s*', '', desc)

    # 删除所属行业说明
    # desc = re.sub(r'所属行业为[^，,。]+[，,]?\s*', '', desc)
    # desc = re.sub(r'(?:所属行业|行业类别)(?:为|是)?[^，,。]+[，,。]?\s*', '', desc)

    # 删除经营状态
    # desc = re.sub(r'企业当前经营状态为[^。]+[。]?\s*', '', desc)
    desc = re.sub(r'(?:企业)?当前经营状态为[^。]+[。]?\s*', '', desc)

    # 清理多余空白和标点
    desc = re.sub(r'\s+', ' ', desc)
    desc = re.sub(r'^[，,。.\s]+', '', desc)
    desc = desc.strip()

    return desc

def extract_qualifications(desc):
    """从企业简介中提取资质标签"""
    result = {
        "专精特新": "否",
        "高新技术企业": "否",
        "独角兽企业": "否",
        "瞪羚企业": "否"
        # "上市企业": ""
    }

    if pd.isna(desc) or not desc:
        return result

    desc = str(desc)

    for qual_name, keywords in QUALIFICATION_KEYWORDS.items():
        for kw in keywords:
            if kw in desc:
                result[qual_name] = "是"
                break

    return result

def extract_special_notes(desc):
    """提取特殊备注（热点领域）"""
    if pd.isna(desc) or not desc:
        return ""

    desc = str(desc)
    notes = []

    for field_name, keywords in SPECIAL_KEYWORDS.items():
        for kw in keywords:
            if kw in desc:
                notes.append(field_name)
                break

    return "、".join(notes) if notes else ""

class ColabSemanticClassifier:
    def __init__(self, model_name: str = MODEL_NAME):
        # 1. 自动检测 GPU
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"正在加载模型: {model_name} | 运行设备: {self.device}")

        # 2. 加载模型 (关键修改：trust_remote_code=True)
        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)

        # 3. 开启半精度 (关键修改：显存减半，速度翻倍)
        if self.device == "cuda":
            self.model.half()
            print(">>> 已开启 FP16 半精度加速")

        self.category_embeddings = {}
        self.category_info = {}
        self._build_category_embeddings()

    def _build_category_embeddings(self):
        print("正在构建分类向量库...")
        texts_to_encode = []
        category_keys = []

        for level1, level2_dict in CLASSIFICATION_SYSTEM.items():
            for level2, info in level2_dict.items():
                key = f"{level1}|{level2}"
                # 4. Stella 优化：为"库向量"添加明确的定义指令
                # 格式：[分类] + 描述
                combined_text = f"行业定义：{level1}-{level2}。业务包含：{info['description']}。关键词：{' '.join(info['keywords'])}"
                texts_to_encode.append(combined_text)
                category_keys.append(key)

                self.category_info[key] = {
                    "level1": level1, "level2": level2,
                    "keywords": info["keywords"]
                }

        # 编码
        embeddings = self.model.encode(texts_to_encode, normalize_embeddings=True, show_progress_bar=False)
        for key, emb in zip(category_keys, embeddings):
            self.category_embeddings[key] = emb
        print(f"分类库构建完成，共 {len(self.category_embeddings)} 个分类")

    def classify_batch(self, texts: List[str], top_k: int = 5, batch_size: int = 128):
        print(f"正在处理 {len(texts)} 条企业数据...")

        # 5. Stella 优化：为"查询向量"添加查询指令
        # 这告诉模型：“我正在寻找这个企业描述对应的分类”
        instructed_texts = []
        for t in texts:
            if not t or pd.isna(t):
                instructed_texts.append("")
            else:
                # 核心 Trick：加上 "查询：" 前缀
                instructed_texts.append(f"查询：该企业的行业分类是？企业描述：{str(t)}")

        # 批量编码
        text_embeddings = self.model.encode(
            instructed_texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True
        )

        # 矩阵计算相似度
        category_keys = list(self.category_embeddings.keys())
        category_matrix = np.array([self.category_embeddings[k] for k in category_keys])
        similarity_matrix = np.dot(text_embeddings, category_matrix.T)

        results = []
        for i, text in enumerate(texts):
            if not text or pd.isna(text):
                results.append([])
                continue

            # 后处理逻辑（保留你的关键词加分机制，这很好）
            text_lower = str(text).lower()
            scores = []
            for j, key in enumerate(category_keys):
                info = self.category_info[key]
                sim = similarity_matrix[i, j]

                kw_bonus = 0
                matched = []
                for kw in info["keywords"]:
                    if kw.lower() in text_lower:
                        kw_bonus += 0.03 # Stella 区分度高，建议降低关键词硬匹配权重，防止干扰
                        matched.append(kw)

                final_score = min(sim + kw_bonus, 1.0)
                scores.append({
                    "level1": info["level1"], "level2": info["level2"],
                    "score": final_score
                })

            scores.sort(key=lambda x: x["score"], reverse=True)
            results.append(scores[:top_k])

        return results

# 辅助选择函数
def select_categories(matches, max_categories=3, min_score=0.4): # Stella 建议提高阈值到 0.4
    selected = []
    used_level1 = set()
    for match in matches:
        if len(selected) >= max_categories: break
        if match["score"] < min_score: continue
        if match["level1"] not in used_level1:
            selected.append(match)
            used_level1.add(match["level1"])
    return selected

def select_subcategory_for_category(matches, category_name, min_score=0.3):
    """
    根据已有的大类名称，从匹配结果中选择最佳的细分类别
    """
    best_match = None
    best_score = min_score
    for match in matches:
        if match["level1"] == category_name and match["score"] >= best_score:
            best_score = match["score"]
            best_match = match
    return best_match

# 设置输入输出文件名
INPUT_FILE = "./enterprises_train.csv"
OUTPUT_FILE = "output_stella.csv"

def main_colab():
    if not os.path.exists(INPUT_FILE):
        print(f"⚠️ 找不到 {INPUT_FILE}。请在左侧文件栏上传你的 CSV 文件。")
        return

    # 1. 读取
    df = pd.read_csv(INPUT_FILE)
    print(f"已读取 {len(df)} 行数据")

    # 2. 初始化分类器
    classifier = ColabSemanticClassifier()

    # 3. 预处理文本
    texts = []
    for idx, row in df.iterrows():
        company = row.get('企业名称', '')
        desc = row.get('企业简介', '')
        scope = row.get('经营范围', '')

        # 清洗
        clean_desc = clean_enterprise_description(desc, company)
        df.at[idx, '企业简介'] = clean_desc # 更新清洗后的简介

        # 拼接用于分类的文本
        combined = f"{clean_desc} {str(scope) if pd.notna(scope) else ''}"
        texts.append(combined)

    # 4. 批量分类 (Batch Size 设为 128 适合 T4/L4 GPU)
    all_matches = classifier.classify_batch(texts, batch_size=64)

    # 5. 填回结果
    print("正在回填数据...")
    for idx, matches in enumerate(all_matches):
        # 获取来源
        source = df.at[idx, '来源'] if '来源' in df.columns else ""
        source = str(source).strip() if pd.notna(source) else ""

        # 判断是否为"入库"或"新增入库"
        is_existing_category = source in ["入库", "新增入库"]

        if is_existing_category:
            # 保留原有类别，只匹配细分类别
            for i in range(3):
                existing_category = df.at[idx, f'类别{i+1}'] if f'类别{i+1}' in df.columns else ""
                existing_category = str(existing_category).strip() if pd.notna(existing_category) else ""

                if existing_category:
                    # 根据已有类别找到最佳细分类
                    best_sub = select_subcategory_for_category(matches, existing_category, min_score=0.3)
                    if best_sub:
                        df.at[idx, f'细分类别{i+1}'] = best_sub['level2']
                        df.at[idx, f'置信度{i+1}'] = round(best_sub['score'], 3)
                    else:
                        # 如果找不到匹配的细分类，保持原值或留空
                        if f'细分类别{i+1}' not in df.columns or pd.isna(df.at[idx, f'细分类别{i+1}']):
                            df.at[idx, f'细分类别{i+1}'] = ""
        else:
            # 正常分类流程
            selected = select_categories(matches, min_score=0.45) # 提高阈值

            # 填充类别
            for i in range(3):
                if i < len(selected):
                    df.at[idx, f'类别{i+1}'] = selected[i]['level1']
                    df.at[idx, f'细分类别{i+1}'] = selected[i]['level2']
                    df.at[idx, f'置信度{i+1}'] = round(selected[i]['score'], 3) # 增加置信度列方便查看
                else:
                    df.at[idx, f'类别{i+1}'] = ""
                    df.at[idx, f'细分类别{i+1}'] = ""

        # 资质提取
        desc = df.at[idx, '企业简介']
        quals = extract_qualifications(desc)
        for k, v in quals.items(): df.at[idx, k] = v

        # 特殊备注
        df.at[idx, '特殊备注'] = extract_special_notes(desc)

    # 6. 保存
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    print(f"✅ 处理完成！结果已保存为: {OUTPUT_FILE}")
    print("请在左侧文件栏下载该文件。")

# 运行
if __name__ == "__main__":
    main_colab()

